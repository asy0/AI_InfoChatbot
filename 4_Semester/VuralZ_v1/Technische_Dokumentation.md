# Technische Dokumentation: FH Technikum Wien Info Chatbot

> **Hinweis:** Dieses Dokument beschreibt den individuellen Prototyp von **Zeliha Vural**. Im Rahmen der Gruppenarbeit (Gruppe 04, 4. Semester, Inno2) hat jedes Gruppenmitglied einen eigenen Prototyp entwickelt.

## Inhaltsverzeichnis
1. [ProjektÃ¼bersicht](#1-projektÃ¼bersicht)
2. [Systemarchitektur](#2-systemarchitektur)
3. [Technische Spezifikationen](#3-technische-spezifikationen)
4. [Installation und Setup](#4-installation-und-setup)
5. [Verwendung](#5-verwendung)
6. [API-Dokumentation](#6-api-dokumentation)
7. [Konfiguration](#7-konfiguration)
8. [Troubleshooting](#8-troubleshooting)
9. [Erweiterungen und Wartung](#9-erweiterungen-und-wartung)
10. [Glossar](#10-glossar)

---

## 1. ProjektÃ¼bersicht

### 1.1 Zweck und Zielsetzung
Das **FH Technikum Wien Info Chatbot**  ist eine automatisierte Bewertungsplattform zur QualitÃ¤tsmessung von Chatbot-Antworten. Das System vergleicht generierte Antworten mit vordefinierten Referenzantworten und bewertet deren Genauigkeit und VollstÃ¤ndigkeit.

### 1.2 Hauptfunktionen
- **Automatisierte Bewertung**: Vergleicht Chatbot-Antworten mit Referenzantworten
- **Vektor-basierte Suche**: Nutzt FAISS fÃ¼r effiziente Ã„hnlichkeitssuche
- **Interaktive BenutzeroberflÃ¤che**: Streamlit-basierte Web-UI
- **Detaillierte Analysen**: Score-Verteilung, Quellenanalyse, Performance-Metriken
- **Export-FunktionalitÃ¤t**: Excel-Export der Bewertungsergebnisse

### 1.3 Technologie-Stack
- **Frontend**: Streamlit
- **Backend**: Python 3.11+
- **NLP**: HuggingFace Transformers, Sentence-Transformers
- **Vektordatenbank**: FAISS
- **Datenverarbeitung**: Pandas, NumPy
- **Machine Learning**: Scikit-learn
- **PDF-Verarbeitung**: PyMuPDF (fitz)

---

## 2. Systemarchitektur

### 2.1 Komponentendiagramm
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI  â”‚    â”‚   PDF Loader    â”‚    â”‚  CSV Test Data  â”‚
â”‚   (Frontend)    â”‚    â”‚   (Data Input)  â”‚    â”‚   (Reference)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Core Engine    â”‚
                    â”‚  (app.py)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FAISS Vector   â”‚    â”‚  Embedding      â”‚    â”‚  Evaluation     â”‚
â”‚  Database       â”‚    â”‚  Model          â”‚    â”‚  Engine         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Datenfluss
1. **Initialisierung**: PDF-Dokumente laden â†’ Text extrahieren â†’ Chunks erstellen
2. **Vektorisierung**: Embeddings generieren â†’ FAISS-Datenbank aufbauen
3. **Evaluation**: Testfragen verarbeiten â†’ Ã„hnlichkeitssuche â†’ Scoring
4. **Ausgabe**: Ergebnisse visualisieren â†’ Export ermÃ¶glichen

### 2.3 SchlÃ¼sselkomponenten

#### 2.3.1 PDF-Verarbeitung (`load_pdf_documents`)
- **Zweck**: Extrahiert Text aus PDF-Dokumenten und erstellt verarbeitbare Chunks
- **Chunking-Strategie**: RecursiveCharacterTextSplitter mit 500 Zeichen Chunks
- **Metadaten**: Speichert Quelle, Seitennummer und Chunk-ID

#### 2.3.2 Vektordatenbank (FAISS)
- **Modell**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimension**: 384-dimensional
- **Suchalgorithmus**: Cosine Similarity
- **Performance**: Optimiert fÃ¼r groÃŸe Dokumentensammlungen

#### 2.3.3 Evaluierungsengine
- **Scoring-Methode**: Cosine Similarity zwischen Embeddings
- **Bewertungskategorien**: VollstÃ¤ndig (>0.75), Teilweise (0.45-0.75), Unzureichend (<0.45)
- **Kontext-Extraktion**: Intelligente Antwort-Extraktion aus gefundenem Text

---

## 3. Technische Spezifikationen

### 3.1 Systemanforderungen
- **Python**: 3.11 oder hÃ¶her
- **RAM**: Mindestens 4GB (8GB empfohlen)
- **Speicherplatz**: 2GB fÃ¼r Modelle und Daten
- **Betriebssystem**: Windows 10+, macOS 10.15+, Linux (Ubuntu 18.04+)

### 3.2 AbhÃ¤ngigkeiten
```python
# Core Dependencies
streamlit>=1.28.0
pandas>=2.0.0
torch>=2.0.0
langchain>=0.1.0
faiss-cpu>=1.7.0
sentence-transformers>=2.2.0
scikit-learn>=1.3.0

# Data Processing
numpy>=1.24.0
PyMuPDF>=1.23.0

# Export & Utilities
xlsxwriter>=3.1.0
openpyxl>=3.1.0
```

### 3.3 Performance-Kennzahlen
- **PDF-Verarbeitung**: ~100 Seiten/Minute
- **Embedding-Generierung**: ~1000 Chunks/Minute
- **Ã„hnlichkeitssuche**: ~100 Queries/Sekunde
- **Speicherverbrauch**: ~2GB fÃ¼r 1000 PDF-Seiten

---

## 4. Installation und Setup

### 4.1 Voraussetzungen
```bash
# Python 3.11+ installieren
python --version

# Virtual Environment erstellen
python -m venv venv

# Virtual Environment aktivieren
# Windows:
venv\Scripts\activate
# Linux/macOS:
source venv/bin/activate
```

### 4.2 Installation
```bash
# AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt

# Oder manuell installieren
pip install streamlit pandas torch langchain faiss-cpu sentence-transformers scikit-learn PyMuPDF xlsxwriter
```

### 4.3 Projektstruktur
```
4_Semester/
â”œâ”€â”€ VuralZ_v1/
â”‚   â”œâ”€â”€ app.py                          # Hauptanwendung
â”‚   â”œâ”€â”€ Technische_Dokumentation.md     # Diese Dokumentation
â”‚   â”œâ”€â”€ testset.csv                     # Testdaten
â”‚   â””â”€â”€ requirements.txt                # AbhÃ¤ngigkeiten
â”œâ”€â”€ data/                              # PDF-Dokumente
â”‚   â”œâ”€â”€ Richtlinie zur Einhebung, RÃ¼ckerstattung, Befreiung von BeitrÃ¤gen_V2.0.pdf
â”‚   â”œâ”€â”€ Hausordnung 2024-05-15_V5.0.pdf
â”‚   â”œâ”€â”€ 5 Satzungsteil Studienrechtliche Bestimmungen PrÃ¼fungsordnung 2024-06-13.pdf
â”‚   â””â”€â”€ Information Ã¼ber die Verwendung personenbezogener Daten von Studierenden.pdf
â”œâ”€â”€ Documentation/                      # Projektdokumentation
â”œâ”€â”€ Sprints/                           # Sprint-Protokolle
â””â”€â”€ venv/                              # Virtual Environment
...
```

---

## 5. Verwendung

### 5.1 Anwendung starten
```bash
# Im Projektverzeichnis
cd 4_Semester/VuralZ_v1

# Streamlit-Anwendung starten
streamlit run app.py
```

### 5.2 BenutzeroberflÃ¤che
1. **Startseite**: Titel und Systemstatus
2. **Verarbeitung**: Automatisches Laden der PDF-Dokumente
3. **Bewertung**: Einzelne Fragen mit Scores und Bewertungen
4. **Zusammenfassung**: Statistiken und Metriken
5. **Export**: Download der Ergebnisse als Excel-Datei

### 5.3 Datenformat
#### CSV-Testdaten (`testset.csv`)
```csv
frage,referenztext
"Wie melde ich mich fÃ¼r eine PrÃ¼fung an?","Die Anmeldung erfolgt Ã¼ber das Online-Portal..."
"Was sind die Zulassungsvoraussetzungen?","FÃ¼r die Zulassung benÃ¶tigen Sie..."
```

#### PDF-Dokumente
- **Format**: PDF 1.4 oder hÃ¶her
- **Sprache**: Deutsch (primÃ¤r)
- **Struktur**: Text-basiert (keine reinen Bilder-PDFs)

---

## 6. API-Dokumentation

### 6.1 Hauptfunktionen

#### `load_pdf_documents(data_folder="../data")`
```python
def load_pdf_documents(data_folder="../data"):
    """
    LÃ¤dt und verarbeitet PDF-Dokumente aus dem angegebenen Ordner.
    
    Args:
        data_folder (str): Pfad zum Ordner mit PDF-Dateien
        
    Returns:
        list: Liste von Document-Objekten mit Text-Chunks
        
    Raises:
        Exception: Bei Fehlern beim Laden der PDF-Dateien
    """
```

#### `find_best_answer(query, referenz, embedding_model, db, k=50)`
```python
def find_best_answer(query, referenz, embedding_model, db, k=50):
    """
    Findet die beste Antwort durch Ã„hnlichkeitssuche.
    
    Args:
        query (str): Suchanfrage
        referenz (str): Referenzantwort
        embedding_model: HuggingFace Embedding-Modell
        db: FAISS-Datenbank
        k (int): Anzahl der zu durchsuchenden Dokumente
        
    Returns:
        tuple: (best_doc, similarity_score)
    """
```

#### `extract_exact_answer(found_text, reference_answer, max_context=200)`
```python
def extract_exact_answer(found_text, reference_answer, max_context=200):
    """
    Extrahiert die exakte Antwort aus dem gefundenen Text.
    
    Args:
        found_text (str): Gefundener Text
        reference_answer (str): Referenzantwort
        max_context (int): Maximale KontextlÃ¤nge
        
    Returns:
        str: Extrahierte Antwort
    """
```

### 6.2 Hilfsfunktionen

#### `clean_text_truncation(text, max_length=1500)`
```python
def clean_text_truncation(text, max_length=1500):
    """
    Schneidet Text sauber ab, ohne mitten im Satz zu enden.
    
    Args:
        text (str): Zu kÃ¼rzender Text
        max_length (int): Maximale LÃ¤nge
        
    Returns:
        str: GekÃ¼rzter Text
    """
```

---

## 7. Konfiguration

### 7.1 Modell-Konfiguration
```python
# Embedding-Modell Ã¤ndern
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"  # Alternative: "all-mpnet-base-v2"
)
```

### 7.2 Bewertungsschwellen anpassen
```python
# In der Hauptlogik (Zeile ~200)
if similarity_score > 0.75:        # VollstÃ¤ndig
    bewertung = "ğŸŸ¢ VollstÃ¤ndig"
elif similarity_score >= 0.45:     # Teilweise
    bewertung = "ğŸŸ¡ Teilweise"
else:                              # Unzureichend
    bewertung = "ğŸ”´ Unzureichend"
```

### 7.3 Chunking-Parameter
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # Chunk-GrÃ¶ÃŸe in Zeichen
    chunk_overlap=100,     # Ãœberlappung zwischen Chunks
    length_function=len,
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

### 7.4 Suchparameter
```python
# Anzahl der zu durchsuchenden Dokumente
k=50  # HÃ¶here Werte = mehr PrÃ¤zision, aber langsamer
```

---

## 8. Troubleshooting

### 8.1 HÃ¤ufige Probleme

#### Problem: "Keine PDF-Dokumente gefunden"
**LÃ¶sung:**
- ÃœberprÃ¼fen Sie den Pfad in `data_folder="../data"`
- Stellen Sie sicher, dass PDF-Dateien im Ordner vorhanden sind
- PrÃ¼fen Sie Dateiberechtigungen

#### Problem: "Out of Memory" Fehler
**LÃ¶sung:**
- Reduzieren Sie `chunk_size` auf 300-400
- Verringern Sie `k` in der Ã„hnlichkeitssuche
- SchlieÃŸen Sie andere Anwendungen

#### Problem: Langsame Performance
**LÃ¶sung:**
- Verwenden Sie GPU-Version von FAISS (`faiss-gpu`)
- Reduzieren Sie die Anzahl der PDF-Dokumente
- Optimieren Sie Chunking-Parameter

### 8.2 Debugging
```python
# Debug-Ausgaben aktivieren
import logging
logging.basicConfig(level=logging.DEBUG)

# Speicherverbrauch Ã¼berwachen
import psutil
print(f"Memory usage: {psutil.Process().memory_info().rss / 1024 / 1024:.2f} MB")
```

### 8.3 Logs und Monitoring
- **Streamlit-Logs**: `streamlit run app.py --logger.level debug`
- **System-Monitoring**: Task Manager / Activity Monitor
- **Performance-Metriken**: In der UI unter "Score-Verteilung"

---

## 9. Erweiterungen und Wartung


### 9.1 Wartungsaufgaben
- **RegelmÃ¤ÃŸige Updates**: HuggingFace-Modelle aktualisieren
- **Performance-Optimierung**: Chunking-Parameter anpassen
- **DatenqualitÃ¤t**: PDF-Dokumente auf Konsistenz prÃ¼fen
- **Backup**: Testdaten und Konfigurationen sichern

### 9.2 Skalierung
```python
# FÃ¼r groÃŸe Dokumentensammlungen
# 1. Chunking optimieren
chunk_size = 1000  # GrÃ¶ÃŸere Chunks
chunk_overlap = 200

# 2. FAISS-Index optimieren
import faiss
index = faiss.IndexFlatIP(384)  # Inner Product fÃ¼r bessere Performance

# 3. Batch-Verarbeitung
def process_batch(queries, batch_size=10):
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]
        # Verarbeite Batch
```

---

## 10. Glossar

### 10.1 Technische Begriffe
- **Embedding**: Vektor-Darstellung von Text in einem hochdimensionalen Raum
- **FAISS**: Facebook AI Similarity Search - Bibliothek fÃ¼r effiziente Ã„hnlichkeitssuche
- **Chunking**: Aufteilung langer Texte in kleinere, verarbeitbare Einheiten
- **Cosine Similarity**: MaÃŸ fÃ¼r die Ã„hnlichkeit zwischen zwei Vektoren
- **Reranking**: NachtrÃ¤gliche Sortierung von Suchergebnissen

### 10.2 Bewertungskategorien
- **VollstÃ¤ndig (ğŸŸ¢)**: Score > 0.75 - Antwort entspricht vollstÃ¤ndig der Referenz
- **Teilweise (ğŸŸ¡)**: Score 0.45-0.75 - Antwort ist teilweise korrekt
- **Unzureichend (ğŸ”´)**: Score < 0.45 - Antwort ist ungenÃ¼gend

### 10.3 Metriken
- **Score**: Ã„hnlichkeitswert zwischen 0 und 1
- **Durchschnittlicher Score**: Arithmetisches Mittel aller Scores
- **Quellenanalyse**: Verteilung der verwendeten Dokumente
- **Performance**: Verarbeitungszeit und Speicherverbrauch

---

## 11. Kontakt und Support

### 11.1 Entwicklerin
- **Projekt**: FH Technikum Wien Info Chatbot
- **Semester**: 4. Semester
- **Gruppe**: Gruppe 04 (Inno2)
- **Entwicklerin**: Zeliha Vural
- **Hinweis**: Individueller Prototyp im Rahmen der Gruppenarbeit - jedes Gruppenmitglied hat einen eigenen Prototyp entwickelt

### 11.2 Dokumentation
- **Version**: 1.0
- **Letzte Aktualisierung**: Juni 2025


### 11.3 Lizenz
Dieses Projekt ist Teil des FH Technikum Wien Curriculums und unterliegt den entsprechenden akademischen Richtlinien.

---

*Diese Dokumentation wird kontinuierlich aktualisiert und erweitert. FÃ¼r Fragen oder VerbesserungsvorschlÃ¤ge wenden Sie sich an Zeliha Vural.* 